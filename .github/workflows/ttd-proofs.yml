name: TTD Proofs â€” build & publish
on:
  push: { branches: [ main ] }
  workflow_dispatch: {}
permissions:
  contents: write
  id-token: write   # required for keyless (Sigstore) signing

jobs:
  proofs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.x' }

      - name: Detect repo files (debug)
        run: |
          echo "== ls -la (root) =="
          ls -la
          echo "== find index.html anywhere (2 levels) =="
          find . -maxdepth 2 -name index.html -print || true
          echo "== find dist (2 levels) =="
          find . -maxdepth 2 -type d -name dist -print || true

      - name: Generate proofs (inline)
        run: |
          python - <<'PY'
          import os, json, hashlib, time
          from pathlib import Path

          ROOT = Path('.')
          OUT  = ROOT / 'proofs'
          OUT.mkdir(exist_ok=True)

          # Try Canvas single-file first; if not present, try Vite dist/*
          candidates = []
          for pat in ['index.html', 'dist/**/*']:
            for p in ROOT.glob(pat):
              if p.is_file():
                candidates.append(p)

          # Deterministic ordering
          files = sorted(set(candidates), key=lambda p: p.as_posix())

          def sha256_file(path: Path):
            import hashlib
            h = hashlib.sha256()
            with open(path, 'rb') as f:
              for chunk in iter(lambda: f.read(1<<20), b''):
                h.update(chunk)
            return h.hexdigest()

          # Write HASHES.txt
          lines, hashes = [], []
          for p in files:
            h = sha256_file(p)
            lines.append(f"{h}  {p.as_posix()} ({p.stat().st_size} bytes)")
            hashes.append((p.as_posix(), h))
          (OUT / 'HASHES.txt').write_text("\n".join(lines) if lines else "No artifacts found.", encoding='utf-8')

          # Merkle (handles 0/1+ leaves)
          def merkle_root(hex_list):
            if not hex_list: return None, []
            lvl = [bytes.fromhex(x) for x in hex_list]
            lvls = [[x.hex() for x in lvl]]
            while len(lvl) > 1:
              nxt = []
              for i in range(0, len(lvl), 2):
                a = lvl[i]; b = lvl[i+1] if i+1 < len(lvl) else lvl[i]
                import hashlib
                nxt.append(hashlib.sha256(a+b).digest())
              lvl = nxt; lvls.append([x.hex() for x in lvl])
            return lvl[0].hex(), lvls

          root, lvls = merkle_root([h for _, h in hashes])
          merkle = {
            "algorithm": "sha256",
            "leaves": [{"file": f, "sha256": h} for f, h in hashes],
            "levels": lvls,
            "merkle_root": root
          }
          (OUT / 'MERKLE.json').write_text(json.dumps(merkle, indent=2), encoding='utf-8')

          # Minimal SBOM + Attestation
          sbom = {
            "bomFormat":"CycloneDX","specVersion":"1.4","version":1,
            "metadata":{"timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())},
            "components":[
              {"type":"library","name":"three","version":"0.158.0","purl":"pkg:npm/three@0.158.0"},
              {"type":"library","name":"vite","version":"^5.0.0","purl":"pkg:npm/vite@5.0.0"}
            ]
          }
          (OUT / 'SBOM.json').write_text(json.dumps(sbom, indent=2), encoding='utf-8')

          att = {
            "_type":"https://in-toto.io/Statement/v1",
            "predicateType":"https://slsa.dev/provenance/v1",
            "subject":[{"name": f, "digest":{"sha256": h}} for f, h in hashes],
            "predicate":{
              "buildType":"https://slsa.dev/container-based-build/v1#vite-gh-pages",
              "builder":{"id":"github-actions://ttd-proofs"},
              "metadata":{"invocationId":f"ttd-proofs-{int(time.time())}"}
            }
          }
          (OUT / 'ATTESTATION.json').write_text(json.dumps(att, indent=2), encoding='utf-8')

          (OUT / 'README.md').write_text(
            "Proofs generated via GitHub Actions. Verify hashes in HASHES.txt.",
            encoding='utf-8'
          )

          print("Files hashed:", [p.as_posix() for p in files])
          PY

      - name: Show proofs (debug)
        run: |
          echo "== proofs directory =="
          ls -la proofs || true
          echo "== HASHES.txt =="
          cat proofs/HASHES.txt || true

      - name: Upload proofs folder (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: ttd_proofs_pack
          path: proofs/**
          if-no-files-found: warn   # don't fail the job if empty

      - name: Pack ZIP for releases (python)
        run: |
          python - <<'PY'
          import os, zipfile
          z=zipfile.ZipFile('ttd_proofs_pack.zip','w',zipfile.ZIP_DEFLATED)
          for root,_,files in os.walk('proofs'):
            for f in files:
              p=os.path.join(root,f)
              z.write(p, arcname=os.path.relpath(p,'proofs'))
          z.close()
          print("Wrote ttd_proofs_pack.zip")
          PY
              - name: Install cosign (Sigstore)
        uses: sigstore/cosign-installer@v3.5.0

      - name: Commit proofs to repo (proofs/)
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(proofs): update proofs pack"
          file_pattern: proofs/**

      - name: Release on tag
        if: startsWith(github.ref, 'refs/tags/')
        uses: softprops/action-gh-release@v2
        with:
          files: ttd_proofs_pack.zip
